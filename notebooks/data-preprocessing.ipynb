{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c95285",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_is_pandas_df' from 'sklearn.utils.validation' (c:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_fraud_data\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_engineering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_time_features\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     prepare_features_for_modeling,\n\u001b[32m      7\u001b[39m     encode_categorical_features,\n\u001b[32m      8\u001b[39m     scale_numerical_features,\n\u001b[32m      9\u001b[39m     handle_class_imbalance,\n\u001b[32m     10\u001b[39m     split_data_stratified\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\notebooks\\..\\src\\data_preprocessing.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler, OneHotEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_pandas_df\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munder_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\imblearn\\__init__.py:54\u001b[39m\n\u001b[32m     50\u001b[39m     sys.stderr.write(\u001b[33m\"\u001b[39m\u001b[33mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     55\u001b[39m         combine,\n\u001b[32m     56\u001b[39m         ensemble,\n\u001b[32m     57\u001b[39m         exceptions,\n\u001b[32m     58\u001b[39m         metrics,\n\u001b[32m     59\u001b[39m         model_selection,\n\u001b[32m     60\u001b[39m         over_sampling,\n\u001b[32m     61\u001b[39m         pipeline,\n\u001b[32m     62\u001b[39m         tensorflow,\n\u001b[32m     63\u001b[39m         under_sampling,\n\u001b[32m     64\u001b[39m         utils,\n\u001b[32m     65\u001b[39m     )\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_smote_enn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_smote_tomek\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mSMOTEENN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSMOTETomek\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\imblearn\\base.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m METHODS\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sklearn_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fit_context, get_tags, validate_data\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\imblearn\\utils\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThe :mod:`imblearn.utils` module includes various utilities.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Substitution\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     check_neighbors_object,\n\u001b[32m      8\u001b[39m     check_sampling_strategy,\n\u001b[32m      9\u001b[39m     check_target_type,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m __all__ = [\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcheck_neighbors_object\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcheck_sampling_strategy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcheck_target_type\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSubstitution\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\imblearn\\utils\\_validation.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m type_of_target\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _num_samples\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sklearn_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_pandas_df, check_array\n\u001b[32m     22\u001b[39m SAMPLING_KIND = (\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mover-sampling\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33munder-sampling\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbypass\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m TARGET_KIND = (\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmultilabel-indicator\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\imblearn\\utils\\_sklearn_compat.py:256\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata_routing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    253\u001b[39m         _raise_for_params,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    254\u001b[39m         process_routing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    255\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    257\u001b[39m         _is_fitted,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    258\u001b[39m         _is_pandas_df,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    259\u001b[39m     )\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m########################################################################################\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# Upgrading for scikit-learn 1.5\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m########################################################################################\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sklearn_version < parse_version(\u001b[33m\"\u001b[39m\u001b[33m1.5\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# chunking\u001b[39;00m\n\u001b[32m    269\u001b[39m     \u001b[38;5;66;03m# extmath\u001b[39;00m\n\u001b[32m    270\u001b[39m     \u001b[38;5;66;03m# fixes\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name '_is_pandas_df' from 'sklearn.utils.validation' (c:\\Users\\arwa\\fraud_detection\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.data_preprocessing import (\n",
    "    prepare_features_for_modeling,\n",
    "    encode_categorical_features,\n",
    "    scale_numerical_features,\n",
    "    handle_class_imbalance,\n",
    "    split_data_stratified\n",
    ")\n",
    "from src.feature_engineering import create_time_features\n",
    "from src.data_loader import load_fraud_data\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21764bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading processed data...\")\n",
    "try:\n",
    "\n",
    "    df = pd.read_csv('../data/processed/fraud_data_with_features.csv',\n",
    "                     parse_dates=['signup_time', 'purchase_time'])\n",
    "    print(f\"Loaded processed data: {df.shape}\")\n",
    "except:\n",
    "    print(\"Processed data not found, creating from scratch...\")\n",
    "    from src.data_loader import load_fraud_data, load_country_mapping\n",
    "    from src.geolocation import add_ip_integer_columns, merge_with_country\n",
    "    from src.feature_engineering import create_time_features\n",
    "\n",
    "    fraud_df = load_fraud_data()\n",
    "    country_df = load_country_mapping()\n",
    "    fraud_df, country_df = add_ip_integer_columns(fraud_df, country_df)\n",
    "    df = merge_with_country(fraud_df, country_df)\n",
    "    df = create_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d58c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n STEP 5: DATA TRANSFORMATION \")\n",
    "print(\"\\n1. Separating features and target...\")\n",
    "X, y, categorical_cols, numerical_cols = prepare_features_for_modeling(df)\n",
    "\n",
    "print(f\"Target variable: class\")\n",
    "print(f\"Categorical features: {categorical_cols}\")\n",
    "print(f\"Numerical features: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. Encoding categorical features...\")\n",
    "X_encoded, encoder = encode_categorical_features(X, categorical_cols)\n",
    "print(f\"Shape after encoding: {X_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dcd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. Scaling numerical features...\")\n",
    "X_scaled, scaler = scale_numerical_features(\n",
    "    X_encoded, numerical_cols, scaler_type='standard')\n",
    "print(f\"Shape after scaling: {X_scaled.shape}\")\n",
    "print(\"\\nSample of scaled numerical features (first 5 rows):\")\n",
    "print(X_scaled[numerical_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. Creating stratified train-test split...\")\n",
    "X_train, X_test, y_train, y_test = split_data_stratified(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n STEP 6: HANDLE CLASS IMBALANCE \")\n",
    "print(\"\\nIMPORTANT: We apply SMOTE ONLY to training data, NOT to test data!\")\n",
    "print(\"\"\"\n",
    "WHY WE CHOOSE SMOTE OVER UNDERSAMPLING:\n",
    "\n",
    "1. SMOTE creates SYNTHETIC minority samples instead of duplicating\n",
    "2. It preserves the information in the majority class (unlike undersampling)\n",
    "3. Fraud detection has VERY FEW positive cases (1.96%)\n",
    "4. With undersampling, we'd lose 98% of our legitimate transactions\n",
    "5. SMOTE helps the model learn better patterns without losing data\n",
    "\n",
    "Alternative considered: Random Under-sampling\n",
    "- Would discard 49 out of 50 legitimate transactions\n",
    "- Too much information loss for fraud detection\n",
    "- Not suitable for such severe imbalance\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "X_train_resampled, y_train_resampled, sampler = handle_class_imbalance(\n",
    "    X_train, y_train, method='smote', random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"\\n Saving Processed Datasets \")\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "datasets = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'X_train_resampled': X_train_resampled,\n",
    "    'y_train_resampled': y_train_resampled\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data.to_csv(f'../data/processed/{name}.csv', index=False)\n",
    "    else:  # Series\n",
    "        data.to_csv(f'../data/processed/{name}.csv', index=False, header=True)\n",
    "    print(f\"Saved {name}: {data.shape}\")\n",
    "\n",
    "print(\"\\nAll datasets saved to ../data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n PREPROCESSING SUMMARY \")\n",
    "print(f\"1. Original data shape: {df.shape}\")\n",
    "print(f\"2. After encoding categorical features: {X_encoded.shape}\")\n",
    "print(f\"3. After scaling numerical features: {X_scaled.shape}\")\n",
    "print(f\"4. Training set (before SMOTE): {X_train.shape}\")\n",
    "print(f\"5. Training set (after SMOTE): {X_train_resampled.shape}\")\n",
    "print(f\"6. Test set (no SMOTE): {X_test.shape}\")\n",
    "\n",
    "print(\"\\nKey points for Task 2 (Model Building):\")\n",
    "print(\"• Use X_train_resampled, y_train_resampled for training\")\n",
    "print(\"• Use X_test, y_test for evaluation (NO SMOTE on test data!)\")\n",
    "print(\"• Test data preserves original class distribution\")\n",
    "print(\"• Training data has balanced classes (50% fraud, 50% legitimate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51518e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n CLASS DISTRIBUTION DOCUMENTATION \")\n",
    "\n",
    "\n",
    "def print_distribution_stats(name, y_data):\n",
    "    total = len(y_data)\n",
    "    fraud = y_data.sum()\n",
    "    legit = total - fraud\n",
    "    fraud_pct = fraud / total * 100\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total samples: {total}\")\n",
    "    print(f\"  Legitimate (0): {legit} ({legit/total*100:.1f}%)\")\n",
    "    print(f\"  Fraudulent (1): {fraud} ({fraud_pct:.1f}%)\")\n",
    "    print(f\"  Imbalance ratio: 1:{int(legit/fraud) if fraud > 0 else 'N/A'}\")\n",
    "\n",
    "\n",
    "print(\"BEFORE RESAMPLING:\")\n",
    "print_distribution_stats(\"Original dataset\", y)\n",
    "print_distribution_stats(\"Training set (before SMOTE)\", y_train)\n",
    "print_distribution_stats(\"Test set\", y_test)\n",
    "\n",
    "print(\"\\nAFTER RESAMPLING:\")\n",
    "print_distribution_stats(\"Training set (after SMOTE)\", y_train_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
